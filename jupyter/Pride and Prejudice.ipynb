{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Pride and Prejudice* by Jane Austen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from find_path import find_dir\n",
    "import os\n",
    "\n",
    "title = \"Pride and Prejudice\"\n",
    "name = \"Pride\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top five words in Pride and Prejudice:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To</td>\n",
       "      <td>4163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Of</td>\n",
       "      <td>3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And</td>\n",
       "      <td>3584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Her</td>\n",
       "      <td>2225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>2070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Word  Count\n",
       "1   To   4163\n",
       "2   Of   3612\n",
       "3  And   3584\n",
       "4  Her   2225\n",
       "5    I   2070"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = find_dir(name)\n",
    "unfiltered = f'{path}/{name}.csv'\n",
    "filtered = f'{path}/{name}_filtered.csv'\n",
    "\n",
    "top_unfiltered = pd.read_csv(unfiltered, names=[\"Word\", \"Count\"]).sort_values([\"Count\", \"Word\"], ascending=False).reset_index(drop=True)\n",
    "top_unfiltered.drop(top_unfiltered.index[0], inplace=True)\n",
    "\n",
    "print(f\"The top five words in {title}:\")\n",
    "top_unfiltered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 ranked words (unfiltered list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.68%\n"
     ]
    }
   ],
   "source": [
    "one_hundred = top_unfiltered[:100]\n",
    "hundred_total = one_hundred[\"Count\"].sum()/ top_unfiltered[\"Count\"].sum()\n",
    "print(f'{str(hundred_total * 100)[:5]}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage overall of the 100 words: 1.59%\n"
     ]
    }
   ],
   "source": [
    "print(f'Percentage overall of the 100 words: {str(((len(one_hundred)/len(top_unfiltered))*100))[:4]}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many words does it take to get to ~80% comprehension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 525 words (rounded to nearest 25) to reach 79.84% comprehension.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>Early</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>Circumstances</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>Call</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>Assured</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>Already</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word  Count\n",
       "521          Early     28\n",
       "522  Circumstances     28\n",
       "523           Call     28\n",
       "524        Assured     28\n",
       "525        Already     28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eighty = top_unfiltered[:525]\n",
    "words = 525\n",
    "eighty_total = eighty[\"Count\"].sum()/ top_unfiltered[\"Count\"].sum()\n",
    "print(f'It takes {words} words (rounded to nearest 25) to reach {str(eighty_total * 100)[:5]}% comprehension.')\n",
    "eighty.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of the 525 words: 8.38%\n"
     ]
    }
   ],
   "source": [
    "print(f'Percentage of the {words} words: {str((len(eighty)/len(top_unfiltered))*100)[:4]}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to transfer that to our filtered list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = pd.read_csv(filtered, names=[\"Word\", \"Count\"]).sort_values([\"Count\", \"Word\"], ascending=False).reset_index(drop=True)\n",
    "top.loc[top['Word']=='Assured']\n",
    "vocab = top[:496]\n",
    "vocab_hundred = vocab[:100]\n",
    "#print(vocab_hundred[\"Word\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons to Modern English words (according to Google's n-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Pride and Prejudice words NOT in 100 Top Modern English Words: \n",
      "\n",
      "['Her', 'She', 'Had', 'Him', 'Could', 'Very', 'Them', 'Said', 'Such', 'Much', 'Must', 'Sister', 'Did', 'Should', 'Know', 'Herself', 'Before', 'Think', 'Soon', 'Never', 'Though', 'Might', 'Well', 'Little', 'Good', 'Most', 'Own', 'Every', 'Then', 'Being', 'Again', 'Friend', 'Without', 'Make', 'Nothing', 'After', 'Room']\n"
     ]
    }
   ],
   "source": [
    "n_path = find_dir('Ngram/edited_n_gram.csv')\n",
    "n_gram = pd.read_csv(n_path, names = [\"drop\", \"Word\", \"Count\"])\n",
    "n_gram.drop([\"drop\"], axis=1, inplace=True)\n",
    "n_gram.drop(index=[0], inplace=True)\n",
    "n_hundred= n_gram[:100]\n",
    "x = vocab_hundred.assign(hundred_in=vocab_hundred.Word.isin(n_hundred.Word).astype(int))\n",
    "h = x[x[\"hundred_in\"] == 0]\n",
    "h = list(h[\"Word\"].unique())\n",
    "print(f'Top 100 {title} words NOT in 100 Top Modern English Words: \\n\\n{h}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pride and Prejudice Vocab List Words NOT in Top 1000 Modern Words (names/locations excluded): \n",
      "\n",
      " ['Sister', 'Herself', 'Soon', 'Nothing', 'Dear', 'Manner', 'Mother', 'Father', 'Daughter', 'Letter', 'Lady', 'Himself', 'Feeling', 'Hope', 'Cannot', 'Saw', 'Marry', 'Felt', 'Wish', 'Quite', 'Myself', 'Attention', 'Pleasure', 'Cry', 'Came', 'Till', 'Heard', 'Aunt', 'Whom', 'Believe', 'Morning', 'Moment', 'Happy', 'Brother', 'Anything', 'Evening', 'Opinion', 'Indeed', 'Therefore', 'Looked', 'Ill', 'Hear', 'Happiness', 'Toward', 'Uncle', 'Told', 'Speak', 'Went', 'Character', 'Seemed', 'Nor', 'Marriage', 'Answer', 'Hour', 'Certainly', 'Leave', 'Kind', 'Gone', 'Conversation', 'Affection', 'Woman', 'Passed', 'Mind', 'Coming', 'Seeing', 'Immediately', 'Cousin', 'Whose', 'Began', 'Knew', 'Almost', 'Perhap', 'Reason', 'Rather', 'Object', 'Behaviour', 'Walk', 'Husband', 'Gave', 'Certain', 'Took', 'Eyes', 'Acquaintance', 'Continued', 'Yourself', 'Idea', 'Civility', 'Suppose', 'Spirit', 'Settled', 'Regard', 'Particular', 'Wife', 'Perfectly', 'Occasion', 'Hardly', 'Satisfy', 'Returned', 'Pride', 'Ought', 'Agreeable', 'Walked', 'Longer', 'Honour', 'Carriage', 'Turned', 'Spoke', 'Scarcely', 'Expected', 'Door', 'Ball', 'Ladyship', 'Impossible', 'Wished', 'Rest', 'Dare', 'Asked', 'Talking', 'Officer', 'Met', 'Gentleman', 'Fortune', 'Former', 'Doubt', 'De', 'Convinced', 'Assure', 'Talked', 'Situation', 'Seem', 'Sat', 'Pleased', 'Likely', 'Gentlemen', 'Connection', 'Wonder', 'Matter', 'Handsome', 'Glad', 'Engaged', 'Dinner', 'Brought', 'Believed', 'Afraid', 'Surprise', 'Sense', 'Expect', 'Entered', 'Compliment', 'Amiable', 'Surprised', 'Saying', 'Respect', 'Relation', 'Invitation', 'Comfort', 'Beyond', 'Wishes', 'Usual', 'Thousand', 'Themselves', 'Sorry', 'Silence', 'Expression', 'Everybody', 'Deal', 'Concern', 'Answered', 'Advantage', 'Speaking', 'Neither', 'Fear', 'Enquiry', 'Appearance', 'Appear', 'Sometimes', 'Sensible', 'Particularly', 'Pain', 'Merely', 'Length', 'Fine', 'Engagement', 'Determined', 'Dance', 'Consequence', 'Afterward', 'Ten', 'Servant', 'Resolved', 'Necessary', 'Instantly', 'Equal', 'Consider', 'Appeared', 'Understand', 'Poor', 'Opportunity', 'Neighbourhood', 'Mention', 'Master', 'Exactly', 'Countenance', 'Arrival', 'Alone', 'Younger', 'Smile', 'Obliged', 'Lost', 'Kindness', 'Forward', 'Astonishment', 'Whatever', 'Satisfaction', 'Mentioned', 'Meant', 'Joined', 'Highly', 'Endeavour', 'Circumstances', 'Attachment']\n"
     ]
    }
   ],
   "source": [
    "y = vocab.assign(vocab_in=vocab.Word.isin(n_gram.Word).astype(int))\n",
    "t = y[y[\"vocab_in\"] == 0]\n",
    "t = list(t[\"Word\"].unique())\n",
    "\n",
    "print(f'{title} Vocab List Words NOT in Top 1000 Modern Words (names/locations excluded): \\n\\n {t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(t).to_csv(f'{path}/{name}_jupyter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab list: 221\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of vocab list: {len(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall word count of book: 118425\n"
     ]
    }
   ],
   "source": [
    "print(f'Overall word count of book: {top_unfiltered[\"Count\"].sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.68 1.6 525 79.85 221 118425\n"
     ]
    }
   ],
   "source": [
    "print(f'{str(round(hundred_total * 100, 2))}', f'{str(round((len(one_hundred)/len(top_unfiltered))*100, 2))}', words, f'{str(round(eighty_total * 100, 2))}', f'{len(t)}', f'{top_unfiltered[\"Count\"].sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = [\"Mouth\", \"Lips\", \"Head\", \"Face\", \"Hair\", \"Eyes\", \"Eye\", \"Ear\", \"Ears\", \"Butt\", \"Ass\", \"Feet\", \"Foot\", \"Leg\", \"Legs\", \"Hand\", \"Hands\"]\n",
    "\n",
    "for part in body:\n",
    "    try:\n",
    "        x = top_unfiltered.loc[top_unfiltered['Word']==part]\n",
    "        print(part, x[\"Count\"])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pride = {\"Mouth/Lips\": 10, \"Head\": 32, \"Face\": 27, \"Hair\": 3, \"Eye(s)\": 67, \"Ear(s)\": 1, \"Foot/Feet\": 2, \"Hand(s)\": 32 }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
